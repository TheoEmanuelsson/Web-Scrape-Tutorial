{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requests to use the get.request function.\n",
    "# BautifulSoup to create the html parser\n",
    "# Pandas for dataframe\n",
    "# Time and random in order to not exhaust website\n",
    "# CSV for saving data\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data columns for saving data into two columns\n",
    "datacolumns1 = ['']\n",
    "datacolumns2 = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A for loop that runs from 0 - 20, where it inputs the value onto the end of our URL.\n",
    "# Every website use a different layout. You need to tweak this for your needs. \n",
    "# In this example I continually add numbers for each loop onto the back of our webpage.\n",
    "# The loops will go through: https://Websiteyouwanttoscrape.io/sectionofwebpage/200000 -> https://Websiteyouwanttoscrape.io/sectionofwebpage/200019\n",
    "# and save the 2nd and 3rd instance of text ('p') it will find.\n",
    "\n",
    "for i in range(0,20):\n",
    "    #Get HTLM-code\n",
    "    response = requests.get('https://Websiteyouwanttoscrape.io/sectionofwebpage/'+ str(200000 + i))\n",
    "    #BeautifulSoup parser\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Makes the loop sleep for a random integer between 5 and 12 seconds for each loop\n",
    "    sleep(randint(5,12))\n",
    "    \n",
    "    # an if-statment that could be defined for anything within the HLTM of the webpage.\n",
    "    # In this case the scraper looks for a class \"heading-secondary\". \n",
    "    # If it does not exit on the webpage it goes to the else-statments,\n",
    "    # that simply does not do anything but continues the loop\n",
    "    \n",
    "    if soup.find_all(class_=\"heading-secondary\"):\n",
    "    \n",
    "        # Selects all text on website\n",
    "        alltext = soup.select('p')\n",
    "     \n",
    "        # Finds the 2nd instance of text on webpage and appends to the data columns created earlier\n",
    "        datacolumn1 = soup.find_all('p')[2].get_text()\n",
    "        datacolumns1.append(datacolumn1)\n",
    "    \n",
    "        # Finds the 3rd instance of text on webpage and appends to the data columns created earlier\n",
    "        datacolumn2 = soup.find_all('p')[3].get_text()\n",
    "        datacolumns2.append(datacolumn2)\n",
    "    \n",
    "        # Create Dataframe\n",
    "        tutorial_df = pd.DataFrame({'datacolumn1': datacolumns1, 'datacolumn2': datacolumns2})\n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Show first 15 rows of dataframe\n",
    "tutorial_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "test_df.to_csv('Datafilename.csv',sep=',', encoding='utf-8', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
